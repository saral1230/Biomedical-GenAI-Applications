{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e354f9-5a87-4910-a600-dc5ccfaf3791",
   "metadata": {},
   "source": [
    "# Previous Module \n",
    "* PubMed Astracts from 5 common dieases catogories were fatched and output to .json files. \n",
    "* These 5 dieases catogories are cancer, chronic, cardiovascular, neurological, infectious.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f06c14b2-2844-4184-b3bf-610987a42112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad19ba4-3a21-4f2b-aac5-278c9ec106f7",
   "metadata": {},
   "source": [
    "# Load PubMed Abstracts \n",
    "Created generator to load those PubMed Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf1d91d-d713-4584-8034-7033a143f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_files(files, path):\n",
    "    for filename in files:\n",
    "        file_loc = os.path.join(path, filename)\n",
    "        with open(file_loc, 'r', encoding='utf-8') as f:\n",
    "            yield json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0178ce-84eb-4555-848c-e5e6e03903e5",
   "metadata": {},
   "source": [
    "# Text Cleanup\n",
    "* Annotated datasets are necessary for Bert model finetuning. \n",
    "* For thosejson files, I only kept the text bodys of abstracts and remove title, author, comments et. al. \n",
    "* Publicly available NER corpus are often on one aspects or multiple categories but not what I expected. When using the chatbox interface of a couple of LLM web services, I observed that the annotation process appeared to be relatively simple. Considering affordbility, I chose  DeepSeek API to annote diseases, genes and chemicals in these abstracts. However, this API doesn't perform well on long text.\n",
    "* Finally, I decided to split all text by sentence and let the API to annotate one sentence at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c1f34c-2be9-4e2e-ab0a-20c5574f19b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_abstract):\n",
    "    author_info_index = None\n",
    "    text = raw_abstract.split('\\n\\n')\n",
    "    \n",
    "    for i, s in enumerate(text):\n",
    "        if s.startswith('Author information:'):\n",
    "            author_info_index = i\n",
    "            break\n",
    "            \n",
    "    if author_info_index is not None:\n",
    "        abstract_body = text[author_info_index + 1]\n",
    "        if abstract_body.startswith(('DOI', 'Comment', 'Publisher', 'BACKGROUND', 'RECENT')):\n",
    "            return None\n",
    "        else: return abstract_body\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def split_by_sentences(text):    \n",
    "    doc = nlp(text)\n",
    "    return [sent.text for sent in doc.sents]\n",
    "\n",
    "def sentences_generator(data):    \n",
    "    for stext in data:        \n",
    "        inputs = stext['abstract']\n",
    "        abs_body = clean_text(inputs)\n",
    "        if abs_body is not None:\n",
    "            text_chunks = split_by_sentences(abs_body)\n",
    "            clean_abs = [t.replace(\"\\n\", \" \") for t in text_chunks]\n",
    "            yield clean_abs\n",
    "        else:\n",
    "            pass\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234254dd-1dbe-4dc2-96be-1f0d50586524",
   "metadata": {},
   "source": [
    "# Text Annotation\n",
    "* Engineer a clear prompt is the key to produce correct annotation\n",
    "* Used generator for optimal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3fcd706-aa60-45c6-982d-696bde51f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_annotation(text, max_retries=3, retry_delay=2):\n",
    "    prompt = f\"\"\"\n",
    "            TASK: Perform biomedical named entity recognition (NER) and relation extraction on the PubMed abstract.\n",
    "            Return ONLY a valid JSON object for the following biomedical text analysis. Do not include any other text, explanations, or markdown formatting.\n",
    "            \n",
    "            TEXT TO PROCESS:\n",
    "            {text}\n",
    "            \n",
    "            INSTRUCTIONS:\n",
    "            INSTRUCTIONS:\n",
    "            1. TOKENIZATION:\n",
    "               - Split text into tokens (whitespace-based)\n",
    "               \n",
    "            \n",
    "            2. ENTITY ANNOTATION (BIO scheme):\n",
    "               - Entity types: gene/protein (B-gene), disease (B-disease), chemical (B-chemical)\n",
    "               - Use \"B-\" for first token, \"I-\" for continuation tokens\n",
    "               - Annotate multi-word entities consistently\n",
    "               - Assign \"0\" for non-entity tokens\n",
    "            \n",
    "            3. RELATION EXTRACTION:\n",
    "               - Relations between adjacent entities only (max distance=3 tokens)\n",
    "               - Valid relation types: \"causes\", \"treats\", \"regulates\", \"associated_with\", \"protein_of\"\n",
    "               - Evidence must be either \"explicit\" or \"implicit\"\n",
    "            \n",
    "            \n",
    "            OUTPUT REQUIREMENTS:\n",
    "            - Strictly valid JSON only (no Markdown, no comments)\n",
    "            - Escape all special characters\n",
    "            - No trailing commas\n",
    "            - Maintain original token order\n",
    "\n",
    "            OUTPUT FORMAT:\n",
    "            {{\n",
    "              \"tokens\": [\"token1\", \"token2\"],\n",
    "              \"ner_tags\": [\"B-gene\", \"0\"],\n",
    "              \"relations\": [\n",
    "                {{\n",
    "                  \"head\": 0,\n",
    "                  \"tail\": 3,\n",
    "                  \"type\": \"regulates\",\n",
    "                  \"evidence\": \"explicit\"\n",
    "                }}\n",
    "              ]\n",
    "            }}\n",
    "            \n",
    "            IMPORTANT:\n",
    "            - If uncertain about an entity/relation, omit it\n",
    "            - Prioritize precision over recall\n",
    "            - Return ONLY the JSON object with no additional text\n",
    "            \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-chat\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Return ONLY valid JSON. No explanations.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.3,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"response_format\": {\"type\": \"json_object\"}\n",
    "    }\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(API_URL, headers=headers, json=payload, timeout=60)\n",
    "            response.raise_for_status()  # Raise HTTP errors (4xx/5xx)\n",
    "            result = response.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except (requests.RequestException, json.JSONDecodeError, KeyError) as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Failed after {max_retries} retries for text (first 50 chars): '{text[:50]}...' | Error: {str(e)}\")\n",
    "                return None\n",
    "            time.sleep(retry_delay * (attempt + 1))  # Exponential backoff\n",
    "\n",
    "def annotate_text_generator(text_chunks):\n",
    "    for i, text in enumerate(text_chunks):\n",
    "        print(f\"Processing text {i+1}/{len(text_chunks)}...\")\n",
    "        annotation_json = text_annotation(text)  # Your annotation function\n",
    "        yield json.loads(annotation_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5613bad-cd60-43b2-9c19-c9d8e73e1e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/your/file/path/data_ouputs'\n",
    "all_files = os.listdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd9fd7f-d0ec-4068-b927-347886cdc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('/your/file/path/.env')\n",
    "API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "API_URL = \"https://api.deepseek.com/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00cb3c8-2279-446b-94c9-1ff4e204adcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "output_file = Path('annotations_gen.jsonl')\n",
    "output_file.unlink(missing_ok=True)  # Clear previous runs \n",
    "\n",
    "data_gen = load_json_files(all_files[2:], file_path)\n",
    "\n",
    "with output_file.open('a', encoding='utf-8') as f:\n",
    "    for data in data_gen:\n",
    "        try:\n",
    "            for sentence in sentences_generator(data):\n",
    "                for annotation in annotate_text_generator(sentence):\n",
    "                    f.write(json.dumps(annotation, ensure_ascii=False) + '\\n')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {data[0]}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7e8c5-ee08-4935-9b4a-df24945ee43d",
   "metadata": {},
   "source": [
    "# Next: PubMedBert Finetune\n",
    "I took advanage of the colab notebook GPU and demo the Finetune here: https://colab.research.google.com/drive/1Fi5rBEabFVF3TlTHqee_-EvYR8Ag00Ak#scrollTo=Zg1CdIIPE4W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74610141-26f7-4cc6-a1b8-7594281dbfe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
